{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0184aa201dc4fa3afe5df2ab71f75732ba4c0e7618a1d5df1129f5ce704f98945",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import preprocess as pr\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "dfc = pd.read_csv(\"data/intents.csv\").sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df2 = dfc[[\"text\",\"intent\"]]\n",
    "\n",
    "pd.isna(df2['text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "pd.isna(df2['text']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dfc.intent.unique().tolist()\n",
    "df2['intent'] = df2['intent'].apply(classes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       2\n",
       "3       3\n",
       "4       4\n",
       "       ..\n",
       "205    22\n",
       "206     8\n",
       "207    16\n",
       "208    30\n",
       "209    22\n",
       "Name: intent, Length: 210, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "df2['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    intent_val                       intent_label\n",
       "0            0                             thanks\n",
       "1            1                    type1_treatment\n",
       "2            2        define_gestational_diabetes\n",
       "3            3                   locate_endocrino\n",
       "4            4                     type1_symptoms\n",
       "5            5              diabetes_risk_factors\n",
       "6            6                    define_diabetes\n",
       "7            7                   change_lifestyle\n",
       "8            8                           greeting\n",
       "9            9              hypoglycemia_symptoms\n",
       "10          10                         define_a1c\n",
       "11          11                      bot_challenge\n",
       "12          12  gestational_diabetes_risk_factors\n",
       "13          13              define_type1_diabetes\n",
       "14          14                 diabetes_treatment\n",
       "15          15                            goodbye\n",
       "16          16                 gestational_causes\n",
       "17          17              define_type2_diabetes\n",
       "18          18                 treat_hypoglycemia\n",
       "19          19                 define_prediabetes\n",
       "20          20                      insulin_types\n",
       "21          21                     type2_symptoms\n",
       "22          22         gestational_affects_babies\n",
       "23          23                define_hypoglycemia\n",
       "24          24                            options\n",
       "25          25                 type2_risk_factors\n",
       "26          26                           noanswer\n",
       "27          27                 type1_risk_factors\n",
       "28          28                  diabetes_symptoms\n",
       "29          29           prediabetes_risk_factors\n",
       "30          30                    type2_treatment\n",
       "31          31                      diabetes_test\n",
       "32          32                 get_diabetes_types"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>intent_val</th>\n      <th>intent_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>thanks</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>type1_treatment</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>define_gestational_diabetes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>locate_endocrino</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>type1_symptoms</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>diabetes_risk_factors</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>define_diabetes</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>change_lifestyle</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>greeting</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>hypoglycemia_symptoms</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>define_a1c</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>bot_challenge</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>gestational_diabetes_risk_factors</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>define_type1_diabetes</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>diabetes_treatment</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>goodbye</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>gestational_causes</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>define_type2_diabetes</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>treat_hypoglycemia</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>define_prediabetes</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>insulin_types</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>type2_symptoms</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>gestational_affects_babies</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>23</td>\n      <td>define_hypoglycemia</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>24</td>\n      <td>options</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>25</td>\n      <td>type2_risk_factors</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>26</td>\n      <td>noanswer</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>27</td>\n      <td>type1_risk_factors</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>28</td>\n      <td>diabetes_symptoms</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>prediabetes_risk_factors</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>30</td>\n      <td>type2_treatment</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>31</td>\n      <td>diabetes_test</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>32</td>\n      <td>get_diabetes_types</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "val_label_cor = pd.concat(\n",
    "    {\n",
    "        \"intent_val\" : df2['intent'],\n",
    "        \"intent_label\" : dfc['intent']\n",
    "    },\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "val_label_cor = val_label_cor.drop_duplicates().reset_index(drop=True)\n",
    "val_label_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(df2[\"text\"].apply(pr.wordvec).to_numpy(), axis=0)\n",
    "y = df2.intent.to_numpy()\n",
    "\n",
    "X_train = X[:int(0.7*X.shape[0])]\n",
    "y_train = y[:int(0.7*y.shape[0])]\n",
    "X_test = X[int(0.7*X.shape[0]):]\n",
    "y_test = y[int(0.7*y.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(63, 33) \n [[0 0 0 ... 0 1 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#preparer les labels\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "def to_vec(val):\n",
    "    vec = [0] * len(classes)\n",
    "    vec[val] = 1\n",
    "    return vec\n",
    "\n",
    "y_train = np.array(list(map(to_vec, y_train)))\n",
    "y_test = np.array(list(map(to_vec, y_test)))\n",
    "\n",
    "print(y_test.shape, '\\n', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " loss: 0.1076 - accuracy: 0.9316 - val_loss: 1.3822 - val_accuracy: 0.7333\n",
      "Epoch 62/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1150 - accuracy: 0.9487 - val_loss: 1.3678 - val_accuracy: 0.7000\n",
      "Epoch 63/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1473 - accuracy: 0.9402 - val_loss: 1.3221 - val_accuracy: 0.7667\n",
      "Epoch 64/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0989 - accuracy: 0.9402 - val_loss: 1.3667 - val_accuracy: 0.6667\n",
      "Epoch 65/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1036 - accuracy: 0.9402 - val_loss: 1.3190 - val_accuracy: 0.7000\n",
      "Epoch 66/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0985 - accuracy: 0.9316 - val_loss: 1.4023 - val_accuracy: 0.7000\n",
      "Epoch 67/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1036 - accuracy: 0.9573 - val_loss: 1.4241 - val_accuracy: 0.7333\n",
      "Epoch 68/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.1051 - accuracy: 0.9487 - val_loss: 1.5035 - val_accuracy: 0.7000\n",
      "Epoch 69/200\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 0.9316 - val_loss: 1.4378 - val_accuracy: 0.6333\n",
      "Epoch 70/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.1125 - accuracy: 0.9231 - val_loss: 1.3979 - val_accuracy: 0.6667\n",
      "Epoch 71/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0839 - accuracy: 0.9402 - val_loss: 1.3771 - val_accuracy: 0.7333\n",
      "Epoch 72/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0915 - accuracy: 0.9402 - val_loss: 1.4047 - val_accuracy: 0.7000\n",
      "Epoch 73/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0891 - accuracy: 0.9402 - val_loss: 1.3961 - val_accuracy: 0.7000\n",
      "Epoch 74/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1067 - accuracy: 0.9487 - val_loss: 1.3652 - val_accuracy: 0.7000\n",
      "Epoch 75/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0881 - accuracy: 0.9316 - val_loss: 1.3265 - val_accuracy: 0.7333\n",
      "Epoch 76/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0903 - accuracy: 0.9487 - val_loss: 1.4871 - val_accuracy: 0.6667\n",
      "Epoch 77/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1052 - accuracy: 0.9487 - val_loss: 1.3785 - val_accuracy: 0.7000\n",
      "Epoch 78/200\n",
      "24/24 [==============================] - 0s 19ms/step - loss: 0.1074 - accuracy: 0.9573 - val_loss: 1.3753 - val_accuracy: 0.6667\n",
      "Epoch 79/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0938 - accuracy: 0.9231 - val_loss: 1.3630 - val_accuracy: 0.7000\n",
      "Epoch 80/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0882 - accuracy: 0.9573 - val_loss: 1.4489 - val_accuracy: 0.7667\n",
      "Epoch 81/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.1037 - accuracy: 0.9487 - val_loss: 1.4417 - val_accuracy: 0.7000\n",
      "Epoch 82/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0870 - accuracy: 0.9402 - val_loss: 1.4091 - val_accuracy: 0.7333\n",
      "Epoch 83/200\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.0931 - accuracy: 0.9402 - val_loss: 1.3979 - val_accuracy: 0.7000\n",
      "Epoch 84/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0882 - accuracy: 0.9402 - val_loss: 1.4373 - val_accuracy: 0.7000\n",
      "Epoch 85/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0891 - accuracy: 0.9316 - val_loss: 1.3866 - val_accuracy: 0.7000\n",
      "Epoch 86/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0936 - accuracy: 0.9402 - val_loss: 1.4451 - val_accuracy: 0.7333\n",
      "Epoch 87/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0985 - accuracy: 0.9487 - val_loss: 1.3922 - val_accuracy: 0.7000\n",
      "Epoch 88/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0982 - accuracy: 0.9487 - val_loss: 1.4306 - val_accuracy: 0.6667\n",
      "Epoch 89/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0867 - accuracy: 0.9402 - val_loss: 1.3702 - val_accuracy: 0.7667\n",
      "Epoch 90/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0800 - accuracy: 0.9402 - val_loss: 1.4079 - val_accuracy: 0.7000\n",
      "Epoch 91/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0877 - accuracy: 0.9316 - val_loss: 1.3697 - val_accuracy: 0.7000\n",
      "Epoch 92/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0939 - accuracy: 0.9316 - val_loss: 1.4076 - val_accuracy: 0.7333\n",
      "Epoch 93/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.1145 - accuracy: 0.9316 - val_loss: 1.4283 - val_accuracy: 0.7333\n",
      "Epoch 94/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0807 - accuracy: 0.9231 - val_loss: 1.4180 - val_accuracy: 0.7333\n",
      "Epoch 95/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0907 - accuracy: 0.9487 - val_loss: 1.4823 - val_accuracy: 0.7333\n",
      "Epoch 96/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0931 - accuracy: 0.9402 - val_loss: 1.4075 - val_accuracy: 0.7333\n",
      "Epoch 97/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1001 - accuracy: 0.9402 - val_loss: 1.3959 - val_accuracy: 0.6667\n",
      "Epoch 98/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0878 - accuracy: 0.9402 - val_loss: 1.3939 - val_accuracy: 0.7333\n",
      "Epoch 99/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0909 - accuracy: 0.9487 - val_loss: 1.4012 - val_accuracy: 0.7333\n",
      "Epoch 100/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0919 - accuracy: 0.9402 - val_loss: 1.4196 - val_accuracy: 0.7000\n",
      "Epoch 101/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0993 - accuracy: 0.9316 - val_loss: 1.4298 - val_accuracy: 0.7000\n",
      "Epoch 102/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0865 - accuracy: 0.9402 - val_loss: 1.4147 - val_accuracy: 0.6667\n",
      "Epoch 103/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0850 - accuracy: 0.9402 - val_loss: 1.3603 - val_accuracy: 0.7000\n",
      "Epoch 104/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0880 - accuracy: 0.9402 - val_loss: 1.3424 - val_accuracy: 0.7333\n",
      "Epoch 105/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0960 - accuracy: 0.9402 - val_loss: 1.3966 - val_accuracy: 0.6667\n",
      "Epoch 106/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0863 - accuracy: 0.9402 - val_loss: 1.4343 - val_accuracy: 0.6667\n",
      "Epoch 107/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0855 - accuracy: 0.9316 - val_loss: 1.3833 - val_accuracy: 0.7000\n",
      "Epoch 108/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0918 - accuracy: 0.9402 - val_loss: 1.4189 - val_accuracy: 0.7000\n",
      "Epoch 109/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0869 - accuracy: 0.9316 - val_loss: 1.4062 - val_accuracy: 0.7000\n",
      "Epoch 110/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0862 - accuracy: 0.9487 - val_loss: 1.4071 - val_accuracy: 0.7333\n",
      "Epoch 111/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0923 - accuracy: 0.9231 - val_loss: 1.4097 - val_accuracy: 0.6667\n",
      "Epoch 112/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0815 - accuracy: 0.9658 - val_loss: 1.3905 - val_accuracy: 0.7333\n",
      "Epoch 113/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0746 - accuracy: 0.9316 - val_loss: 1.4064 - val_accuracy: 0.7333\n",
      "Epoch 114/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0774 - accuracy: 0.9316 - val_loss: 1.4125 - val_accuracy: 0.7333\n",
      "Epoch 115/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0917 - accuracy: 0.9402 - val_loss: 1.3986 - val_accuracy: 0.7000\n",
      "Epoch 116/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0832 - accuracy: 0.9316 - val_loss: 1.4027 - val_accuracy: 0.7333\n",
      "Epoch 117/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0879 - accuracy: 0.9231 - val_loss: 1.4054 - val_accuracy: 0.6667\n",
      "Epoch 118/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0837 - accuracy: 0.9316 - val_loss: 1.4091 - val_accuracy: 0.7333\n",
      "Epoch 119/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0828 - accuracy: 0.9316 - val_loss: 1.4072 - val_accuracy: 0.7000\n",
      "Epoch 120/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0892 - accuracy: 0.9316 - val_loss: 1.4369 - val_accuracy: 0.7333\n",
      "Epoch 121/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0783 - accuracy: 0.9402 - val_loss: 1.4058 - val_accuracy: 0.7333\n",
      "Epoch 122/200\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0793 - accuracy: 0.9573 - val_loss: 1.4120 - val_accuracy: 0.7000\n",
      "Epoch 123/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0808 - accuracy: 0.9402 - val_loss: 1.4123 - val_accuracy: 0.7333\n",
      "Epoch 124/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0866 - accuracy: 0.9316 - val_loss: 1.4801 - val_accuracy: 0.7000\n",
      "Epoch 125/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0797 - accuracy: 0.9402 - val_loss: 1.4293 - val_accuracy: 0.7000\n",
      "Epoch 126/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0835 - accuracy: 0.9487 - val_loss: 1.3914 - val_accuracy: 0.7333\n",
      "Epoch 127/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0773 - accuracy: 0.9316 - val_loss: 1.4251 - val_accuracy: 0.7333\n",
      "Epoch 128/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0946 - accuracy: 0.9573 - val_loss: 1.3601 - val_accuracy: 0.7000\n",
      "Epoch 129/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0766 - accuracy: 0.9658 - val_loss: 1.4543 - val_accuracy: 0.6667\n",
      "Epoch 130/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0798 - accuracy: 0.9316 - val_loss: 1.4404 - val_accuracy: 0.7333\n",
      "Epoch 131/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0815 - accuracy: 0.9487 - val_loss: 1.4407 - val_accuracy: 0.7333\n",
      "Epoch 132/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0818 - accuracy: 0.9573 - val_loss: 1.4419 - val_accuracy: 0.6667\n",
      "Epoch 133/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0824 - accuracy: 0.9487 - val_loss: 1.4243 - val_accuracy: 0.7333\n",
      "Epoch 134/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0816 - accuracy: 0.9402 - val_loss: 1.4470 - val_accuracy: 0.7000\n",
      "Epoch 135/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0780 - accuracy: 0.9231 - val_loss: 1.4096 - val_accuracy: 0.7333\n",
      "Epoch 136/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0815 - accuracy: 0.9402 - val_loss: 1.4369 - val_accuracy: 0.7000\n",
      "Epoch 137/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0900 - accuracy: 0.9487 - val_loss: 1.4718 - val_accuracy: 0.7333\n",
      "Epoch 138/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0895 - accuracy: 0.9402 - val_loss: 1.4299 - val_accuracy: 0.7333\n",
      "Epoch 139/200\n",
      "24/24 [==============================] - 0s 14ms/step - loss: 0.0769 - accuracy: 0.9573 - val_loss: 1.4650 - val_accuracy: 0.7000\n",
      "Epoch 140/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0752 - accuracy: 0.9402 - val_loss: 1.4287 - val_accuracy: 0.7333\n",
      "Epoch 141/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0777 - accuracy: 0.9316 - val_loss: 1.4171 - val_accuracy: 0.7000\n",
      "Epoch 142/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1037 - accuracy: 0.9487 - val_loss: 1.4131 - val_accuracy: 0.7333\n",
      "Epoch 143/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0780 - accuracy: 0.9402 - val_loss: 1.4551 - val_accuracy: 0.7667\n",
      "Epoch 144/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0759 - accuracy: 0.9487 - val_loss: 1.4018 - val_accuracy: 0.7333\n",
      "Epoch 145/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0731 - accuracy: 0.9316 - val_loss: 1.3951 - val_accuracy: 0.7333\n",
      "Epoch 146/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0776 - accuracy: 0.9487 - val_loss: 1.4350 - val_accuracy: 0.7333\n",
      "Epoch 147/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0754 - accuracy: 0.9487 - val_loss: 1.4366 - val_accuracy: 0.7000\n",
      "Epoch 148/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.1004 - accuracy: 0.9402 - val_loss: 1.4265 - val_accuracy: 0.7667\n",
      "Epoch 149/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0751 - accuracy: 0.9487 - val_loss: 1.4306 - val_accuracy: 0.7333\n",
      "Epoch 150/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0794 - accuracy: 0.9487 - val_loss: 1.4293 - val_accuracy: 0.7333\n",
      "Epoch 151/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0722 - accuracy: 0.9402 - val_loss: 1.4200 - val_accuracy: 0.7333\n",
      "Epoch 152/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0734 - accuracy: 0.9487 - val_loss: 1.4546 - val_accuracy: 0.7333\n",
      "Epoch 153/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0748 - accuracy: 0.9402 - val_loss: 1.4482 - val_accuracy: 0.7333\n",
      "Epoch 154/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0810 - accuracy: 0.9402 - val_loss: 1.4620 - val_accuracy: 0.7333\n",
      "Epoch 155/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0777 - accuracy: 0.9487 - val_loss: 1.3826 - val_accuracy: 0.7333\n",
      "Epoch 156/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0823 - accuracy: 0.9573 - val_loss: 1.4432 - val_accuracy: 0.7333\n",
      "Epoch 157/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0769 - accuracy: 0.9316 - val_loss: 1.4720 - val_accuracy: 0.7333\n",
      "Epoch 158/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0804 - accuracy: 0.9573 - val_loss: 1.4694 - val_accuracy: 0.7333\n",
      "Epoch 159/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0984 - accuracy: 0.9402 - val_loss: 1.3855 - val_accuracy: 0.7333\n",
      "Epoch 160/200\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.0751 - accuracy: 0.9658 - val_loss: 1.5039 - val_accuracy: 0.7333\n",
      "Epoch 161/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0836 - accuracy: 0.9487 - val_loss: 1.4180 - val_accuracy: 0.7333\n",
      "Epoch 162/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0752 - accuracy: 0.9402 - val_loss: 1.4532 - val_accuracy: 0.7333\n",
      "Epoch 163/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0721 - accuracy: 0.9487 - val_loss: 1.4842 - val_accuracy: 0.7333\n",
      "Epoch 164/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0759 - accuracy: 0.9402 - val_loss: 1.4754 - val_accuracy: 0.7333\n",
      "Epoch 165/200\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.0750 - accuracy: 0.9231 - val_loss: 1.4825 - val_accuracy: 0.7333\n",
      "Epoch 166/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0760 - accuracy: 0.9231 - val_loss: 1.4557 - val_accuracy: 0.7333\n",
      "Epoch 167/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0880 - accuracy: 0.9402 - val_loss: 1.4826 - val_accuracy: 0.7000\n",
      "Epoch 168/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0752 - accuracy: 0.9402 - val_loss: 1.4649 - val_accuracy: 0.7333\n",
      "Epoch 169/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0750 - accuracy: 0.9402 - val_loss: 1.4695 - val_accuracy: 0.7333\n",
      "Epoch 170/200\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.0820 - accuracy: 0.9573 - val_loss: 1.4492 - val_accuracy: 0.7333\n",
      "Epoch 171/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0762 - accuracy: 0.9402 - val_loss: 1.4349 - val_accuracy: 0.7333\n",
      "Epoch 172/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0740 - accuracy: 0.9487 - val_loss: 1.4675 - val_accuracy: 0.7333\n",
      "Epoch 173/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0710 - accuracy: 0.9573 - val_loss: 1.4950 - val_accuracy: 0.7333\n",
      "Epoch 174/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0839 - accuracy: 0.9487 - val_loss: 1.4873 - val_accuracy: 0.7333\n",
      "Epoch 175/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0744 - accuracy: 0.9402 - val_loss: 1.4679 - val_accuracy: 0.7333\n",
      "Epoch 176/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0733 - accuracy: 0.9487 - val_loss: 1.4760 - val_accuracy: 0.7333\n",
      "Epoch 177/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0747 - accuracy: 0.9573 - val_loss: 1.4474 - val_accuracy: 0.7333\n",
      "Epoch 178/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0784 - accuracy: 0.9316 - val_loss: 1.4496 - val_accuracy: 0.7333\n",
      "Epoch 179/200\n",
      "24/24 [==============================] - 0s 12ms/step - loss: 0.0719 - accuracy: 0.9316 - val_loss: 1.4626 - val_accuracy: 0.7000\n",
      "Epoch 180/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0767 - accuracy: 0.9316 - val_loss: 1.4452 - val_accuracy: 0.7333\n",
      "Epoch 181/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0780 - accuracy: 0.9402 - val_loss: 1.4472 - val_accuracy: 0.7333\n",
      "Epoch 182/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9487 - val_loss: 1.4585 - val_accuracy: 0.7333\n",
      "Epoch 183/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0725 - accuracy: 0.9487 - val_loss: 1.4511 - val_accuracy: 0.7333\n",
      "Epoch 184/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0726 - accuracy: 0.9402 - val_loss: 1.4834 - val_accuracy: 0.7333\n",
      "Epoch 185/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9487 - val_loss: 1.4795 - val_accuracy: 0.7333\n",
      "Epoch 186/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0766 - accuracy: 0.9573 - val_loss: 1.4966 - val_accuracy: 0.7333\n",
      "Epoch 187/200\n",
      "24/24 [==============================] - 0s 13ms/step - loss: 0.0704 - accuracy: 0.9402 - val_loss: 1.4467 - val_accuracy: 0.7333\n",
      "Epoch 188/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0747 - accuracy: 0.9402 - val_loss: 1.4404 - val_accuracy: 0.7333\n",
      "Epoch 189/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0796 - accuracy: 0.9573 - val_loss: 1.4469 - val_accuracy: 0.7333\n",
      "Epoch 190/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0692 - accuracy: 0.9487 - val_loss: 1.4531 - val_accuracy: 0.7333\n",
      "Epoch 191/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0692 - accuracy: 0.9402 - val_loss: 1.4728 - val_accuracy: 0.7333\n",
      "Epoch 192/200\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.0761 - accuracy: 0.9316 - val_loss: 1.4704 - val_accuracy: 0.7333\n",
      "Epoch 193/200\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.0938 - accuracy: 0.9573 - val_loss: 1.4215 - val_accuracy: 0.7000\n",
      "Epoch 194/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0785 - accuracy: 0.9487 - val_loss: 1.4219 - val_accuracy: 0.7333\n",
      "Epoch 195/200\n",
      "24/24 [==============================] - 0s 11ms/step - loss: 0.0752 - accuracy: 0.9402 - val_loss: 1.4639 - val_accuracy: 0.7000\n",
      "Epoch 196/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0755 - accuracy: 0.9487 - val_loss: 1.4585 - val_accuracy: 0.7333\n",
      "Epoch 197/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0806 - accuracy: 0.9573 - val_loss: 1.4416 - val_accuracy: 0.7333\n",
      "Epoch 198/200\n",
      "24/24 [==============================] - 0s 9ms/step - loss: 0.0741 - accuracy: 0.9487 - val_loss: 1.4132 - val_accuracy: 0.7333\n",
      "Epoch 199/200\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.0757 - accuracy: 0.9231 - val_loss: 1.4474 - val_accuracy: 0.7333\n",
      "Epoch 200/200\n",
      "24/24 [==============================] - 0s 10ms/step - loss: 0.0733 - accuracy: 0.9402 - val_loss: 1.4522 - val_accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "#Definissons un modele sequentiel Keras de 3 couches\n",
    "test_model = Sequential()\n",
    "test_model.add(Dense(300, input_dim=300, activation='relu')) #dim des vecteurs vaut 300\n",
    "test_model.add(Dense(150, activation='relu'))\n",
    "test_model.add(Dense(y_train[0].size, activation='sigmoid'))\n",
    "test_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Le gradient descent pour optimiser\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "test_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "hist = test_model.fit(X_train, y_train, epochs=200, validation_split=0.2, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 2.3582 - accuracy: 0.6190\n",
      "Accuracy: 61.90\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = test_model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "source": [
    "## Hyperparameter tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=(X_train.shape[1],)))\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=300, max_value=1000, step=10)\n",
    "  activ = hp.Choice('activation', values = ['relu', 'sigmoid', 'softmax', 'tanh'])\n",
    "  model.add(Dense(units=hp_units, activation=activ))\n",
    "  \n",
    "  #2nd layer\n",
    "  hp_units2 = hp.Int('units2', min_value=30, max_value = hp_units, step = 10)\n",
    "  activ2 = hp.Choice('activation2', values = ['relu', 'sigmoid', 'softmax', 'tanh'])\n",
    "  model.add(Dense(units=hp_units2, activation=activ2))\n",
    "  #output layer \n",
    "  model.add(Dense(len(classes), activation='sigmoid'))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     factor=3,\n",
    "                     directory='keras-model',\n",
    "                     project_name='hyperpara_tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial 90 Complete [00h 00m 13s]\nval_accuracy: 0.800000011920929\n\nBest val_accuracy So Far: 0.800000011920929\nTotal elapsed time: 00h 07m 31s\nINFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=200, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "meilleur units 1 : 440\nmeilleur activation 1 : tanh\nmeilleur units 2 : 90\nmeilleur activation 1 : tanh\nmeilleur learning rate : 0.001.\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"meilleur units 1 : {best_hps.get('units')}\\nmeilleur activation 1 : {best_hps.get('activation')}\\nmeilleur units 2 : {best_hps.get('units2')}\\nmeilleur activation 1 : {best_hps.get('activation2')}\\nmeilleur learning rate : {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "och 60/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.1213 - accuracy: 0.9487 - val_loss: 1.2522 - val_accuracy: 0.7333\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.1186 - accuracy: 0.9573 - val_loss: 1.2602 - val_accuracy: 0.7333\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.1159 - accuracy: 0.9487 - val_loss: 1.2470 - val_accuracy: 0.7333\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1129 - accuracy: 0.9573 - val_loss: 1.2402 - val_accuracy: 0.7333\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.1098 - accuracy: 0.9573 - val_loss: 1.2386 - val_accuracy: 0.7333\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1096 - accuracy: 0.9573 - val_loss: 1.2412 - val_accuracy: 0.7333\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.1064 - accuracy: 0.9573 - val_loss: 1.2355 - val_accuracy: 0.7333\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.1054 - accuracy: 0.9573 - val_loss: 1.2297 - val_accuracy: 0.7333\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.1031 - accuracy: 0.9573 - val_loss: 1.2266 - val_accuracy: 0.7333\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.1023 - accuracy: 0.9402 - val_loss: 1.2280 - val_accuracy: 0.7333\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.1024 - accuracy: 0.9487 - val_loss: 1.2287 - val_accuracy: 0.7333\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0989 - accuracy: 0.9573 - val_loss: 1.2202 - val_accuracy: 0.7333\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0981 - accuracy: 0.9573 - val_loss: 1.2264 - val_accuracy: 0.7333\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0976 - accuracy: 0.9573 - val_loss: 1.2319 - val_accuracy: 0.7333\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0987 - accuracy: 0.9316 - val_loss: 1.2311 - val_accuracy: 0.7333\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0956 - accuracy: 0.9573 - val_loss: 1.2416 - val_accuracy: 0.7333\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0960 - accuracy: 0.9573 - val_loss: 1.2460 - val_accuracy: 0.7000\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0943 - accuracy: 0.9487 - val_loss: 1.2396 - val_accuracy: 0.7000\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0932 - accuracy: 0.9487 - val_loss: 1.2362 - val_accuracy: 0.7000\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0907 - accuracy: 0.9573 - val_loss: 1.2331 - val_accuracy: 0.7000\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0915 - accuracy: 0.9487 - val_loss: 1.2348 - val_accuracy: 0.7333\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0904 - accuracy: 0.9487 - val_loss: 1.2268 - val_accuracy: 0.7000\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0882 - accuracy: 0.9573 - val_loss: 1.2264 - val_accuracy: 0.7000\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0886 - accuracy: 0.9402 - val_loss: 1.2358 - val_accuracy: 0.7000\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0890 - accuracy: 0.9402 - val_loss: 1.2422 - val_accuracy: 0.7000\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0880 - accuracy: 0.9487 - val_loss: 1.2432 - val_accuracy: 0.7000\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0860 - accuracy: 0.9487 - val_loss: 1.2395 - val_accuracy: 0.7000\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0856 - accuracy: 0.9402 - val_loss: 1.2373 - val_accuracy: 0.7000\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0847 - accuracy: 0.9487 - val_loss: 1.2381 - val_accuracy: 0.7000\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0863 - accuracy: 0.9402 - val_loss: 1.2332 - val_accuracy: 0.7000\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0835 - accuracy: 0.9573 - val_loss: 1.2447 - val_accuracy: 0.7000\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0838 - accuracy: 0.9402 - val_loss: 1.2482 - val_accuracy: 0.7000\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0830 - accuracy: 0.9487 - val_loss: 1.2487 - val_accuracy: 0.7000\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0827 - accuracy: 0.9402 - val_loss: 1.2451 - val_accuracy: 0.7000\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0813 - accuracy: 0.9487 - val_loss: 1.2432 - val_accuracy: 0.7000\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0820 - accuracy: 0.9573 - val_loss: 1.2425 - val_accuracy: 0.7000\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0821 - accuracy: 0.9487 - val_loss: 1.2361 - val_accuracy: 0.7000\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0821 - accuracy: 0.9487 - val_loss: 1.2345 - val_accuracy: 0.7000\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0840 - accuracy: 0.9316 - val_loss: 1.2530 - val_accuracy: 0.7000\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0807 - accuracy: 0.9573 - val_loss: 1.2511 - val_accuracy: 0.7000\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 0.0805 - accuracy: 0.9573 - val_loss: 1.2440 - val_accuracy: 0.7000\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0829 - accuracy: 0.9573 - val_loss: 1.2385 - val_accuracy: 0.7000\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0815 - accuracy: 0.9573 - val_loss: 1.2368 - val_accuracy: 0.7000\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0803 - accuracy: 0.9487 - val_loss: 1.2506 - val_accuracy: 0.7000\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0827 - accuracy: 0.9487 - val_loss: 1.2737 - val_accuracy: 0.7000\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0811 - accuracy: 0.9573 - val_loss: 1.2636 - val_accuracy: 0.7000\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0782 - accuracy: 0.9487 - val_loss: 1.2628 - val_accuracy: 0.7000\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0765 - accuracy: 0.9573 - val_loss: 1.2628 - val_accuracy: 0.7000\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0788 - accuracy: 0.9573 - val_loss: 1.2636 - val_accuracy: 0.7000\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0793 - accuracy: 0.9573 - val_loss: 1.2608 - val_accuracy: 0.7000\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0769 - accuracy: 0.9573 - val_loss: 1.2694 - val_accuracy: 0.7000\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0762 - accuracy: 0.9487 - val_loss: 1.2663 - val_accuracy: 0.7000\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0778 - accuracy: 0.9487 - val_loss: 1.2587 - val_accuracy: 0.7000\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0776 - accuracy: 0.9573 - val_loss: 1.2636 - val_accuracy: 0.7000\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0767 - accuracy: 0.9487 - val_loss: 1.2616 - val_accuracy: 0.7000\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0763 - accuracy: 0.9402 - val_loss: 1.2558 - val_accuracy: 0.7000\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0784 - accuracy: 0.9573 - val_loss: 1.2451 - val_accuracy: 0.7000\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0756 - accuracy: 0.9573 - val_loss: 1.2430 - val_accuracy: 0.7000\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0740 - accuracy: 0.9573 - val_loss: 1.2572 - val_accuracy: 0.7000\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0764 - accuracy: 0.9573 - val_loss: 1.2757 - val_accuracy: 0.7000\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0762 - accuracy: 0.9402 - val_loss: 1.2758 - val_accuracy: 0.7000\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0751 - accuracy: 0.9487 - val_loss: 1.2791 - val_accuracy: 0.7000\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0732 - accuracy: 0.9573 - val_loss: 1.2735 - val_accuracy: 0.7000\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0730 - accuracy: 0.9487 - val_loss: 1.2669 - val_accuracy: 0.7000\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.0740 - accuracy: 0.9487 - val_loss: 1.2606 - val_accuracy: 0.7000\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0738 - accuracy: 0.9573 - val_loss: 1.2588 - val_accuracy: 0.7000\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0757 - accuracy: 0.9573 - val_loss: 1.2581 - val_accuracy: 0.7000\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0743 - accuracy: 0.9487 - val_loss: 1.2610 - val_accuracy: 0.6667\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0724 - accuracy: 0.9573 - val_loss: 1.2768 - val_accuracy: 0.6667\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0728 - accuracy: 0.9573 - val_loss: 1.2898 - val_accuracy: 0.6667\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0725 - accuracy: 0.9487 - val_loss: 1.2926 - val_accuracy: 0.6667\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0734 - accuracy: 0.9573 - val_loss: 1.2858 - val_accuracy: 0.6667\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0718 - accuracy: 0.9487 - val_loss: 1.2866 - val_accuracy: 0.7000\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0729 - accuracy: 0.9573 - val_loss: 1.2862 - val_accuracy: 0.6667\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0726 - accuracy: 0.9402 - val_loss: 1.2839 - val_accuracy: 0.6667\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0725 - accuracy: 0.9487 - val_loss: 1.2781 - val_accuracy: 0.6667\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0720 - accuracy: 0.9573 - val_loss: 1.2807 - val_accuracy: 0.6667\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.0715 - accuracy: 0.9658 - val_loss: 1.2785 - val_accuracy: 0.6667\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0752 - accuracy: 0.9487 - val_loss: 1.2663 - val_accuracy: 0.6667\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0724 - accuracy: 0.9573 - val_loss: 1.2750 - val_accuracy: 0.6667\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0715 - accuracy: 0.9573 - val_loss: 1.2793 - val_accuracy: 0.6667\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.0712 - accuracy: 0.9573 - val_loss: 1.2866 - val_accuracy: 0.6667\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0738 - accuracy: 0.9487 - val_loss: 1.3015 - val_accuracy: 0.6667\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0711 - accuracy: 0.9573 - val_loss: 1.2992 - val_accuracy: 0.6667\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0704 - accuracy: 0.9573 - val_loss: 1.2977 - val_accuracy: 0.6667\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0718 - accuracy: 0.9487 - val_loss: 1.3004 - val_accuracy: 0.6667\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 47ms/step - loss: 0.0710 - accuracy: 0.9487 - val_loss: 1.3012 - val_accuracy: 0.6667\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0742 - accuracy: 0.9402 - val_loss: 1.2850 - val_accuracy: 0.6667\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0704 - accuracy: 0.9573 - val_loss: 1.2834 - val_accuracy: 0.6667\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0703 - accuracy: 0.9573 - val_loss: 1.2886 - val_accuracy: 0.6667\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0727 - accuracy: 0.9231 - val_loss: 1.2998 - val_accuracy: 0.6667\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0689 - accuracy: 0.9487 - val_loss: 1.2980 - val_accuracy: 0.6667\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0711 - accuracy: 0.9231 - val_loss: 1.2853 - val_accuracy: 0.6667\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0711 - accuracy: 0.9573 - val_loss: 1.2844 - val_accuracy: 0.6667\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0690 - accuracy: 0.9573 - val_loss: 1.2910 - val_accuracy: 0.7000\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0693 - accuracy: 0.9487 - val_loss: 1.3006 - val_accuracy: 0.7000\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0694 - accuracy: 0.9573 - val_loss: 1.3047 - val_accuracy: 0.7000\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0697 - accuracy: 0.9573 - val_loss: 1.3077 - val_accuracy: 0.6667\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0684 - accuracy: 0.9487 - val_loss: 1.3124 - val_accuracy: 0.6667\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 0.0705 - accuracy: 0.9487 - val_loss: 1.3088 - val_accuracy: 0.6667\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0685 - accuracy: 0.9573 - val_loss: 1.3099 - val_accuracy: 0.6667\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0697 - accuracy: 0.9573 - val_loss: 1.3170 - val_accuracy: 0.7000\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0685 - accuracy: 0.9487 - val_loss: 1.3043 - val_accuracy: 0.7000\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0700 - accuracy: 0.9573 - val_loss: 1.2968 - val_accuracy: 0.7000\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0682 - accuracy: 0.9573 - val_loss: 1.2984 - val_accuracy: 0.7000\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 37ms/step - loss: 0.0677 - accuracy: 0.9487 - val_loss: 1.3104 - val_accuracy: 0.7000\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0679 - accuracy: 0.9402 - val_loss: 1.3131 - val_accuracy: 0.7000\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0700 - accuracy: 0.9487 - val_loss: 1.3279 - val_accuracy: 0.7000\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 50ms/step - loss: 0.0694 - accuracy: 0.9573 - val_loss: 1.3287 - val_accuracy: 0.7000\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0686 - accuracy: 0.9573 - val_loss: 1.3198 - val_accuracy: 0.7000\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0689 - accuracy: 0.9402 - val_loss: 1.3142 - val_accuracy: 0.6667\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0667 - accuracy: 0.9573 - val_loss: 1.3145 - val_accuracy: 0.6667\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 35ms/step - loss: 0.0683 - accuracy: 0.9573 - val_loss: 1.3136 - val_accuracy: 0.6667\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.0677 - accuracy: 0.9487 - val_loss: 1.3162 - val_accuracy: 0.6667\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0671 - accuracy: 0.9573 - val_loss: 1.3160 - val_accuracy: 0.7000\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0702 - accuracy: 0.9231 - val_loss: 1.3216 - val_accuracy: 0.7000\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0670 - accuracy: 0.9573 - val_loss: 1.3267 - val_accuracy: 0.7000\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0688 - accuracy: 0.9402 - val_loss: 1.3246 - val_accuracy: 0.7000\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0669 - accuracy: 0.9573 - val_loss: 1.3252 - val_accuracy: 0.7000\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0672 - accuracy: 0.9487 - val_loss: 1.3239 - val_accuracy: 0.7000\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.0674 - accuracy: 0.9487 - val_loss: 1.3251 - val_accuracy: 0.7000\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0664 - accuracy: 0.9573 - val_loss: 1.3219 - val_accuracy: 0.7000\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 48ms/step - loss: 0.0672 - accuracy: 0.9402 - val_loss: 1.3236 - val_accuracy: 0.7000\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 45ms/step - loss: 0.0678 - accuracy: 0.9402 - val_loss: 1.3208 - val_accuracy: 0.7000\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0658 - accuracy: 0.9573 - val_loss: 1.3218 - val_accuracy: 0.7000\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0662 - accuracy: 0.9573 - val_loss: 1.3263 - val_accuracy: 0.7000\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0664 - accuracy: 0.9573 - val_loss: 1.3312 - val_accuracy: 0.7000\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0684 - accuracy: 0.9573 - val_loss: 1.3278 - val_accuracy: 0.7000\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0681 - accuracy: 0.9573 - val_loss: 1.3362 - val_accuracy: 0.7000\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.0668 - accuracy: 0.9402 - val_loss: 1.3339 - val_accuracy: 0.7000\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0664 - accuracy: 0.9402 - val_loss: 1.3321 - val_accuracy: 0.7000\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0667 - accuracy: 0.9487 - val_loss: 1.3302 - val_accuracy: 0.7000\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0657 - accuracy: 0.9573 - val_loss: 1.3298 - val_accuracy: 0.7000\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0660 - accuracy: 0.9573 - val_loss: 1.3359 - val_accuracy: 0.7000\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0666 - accuracy: 0.9487 - val_loss: 1.3390 - val_accuracy: 0.7000\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.0662 - accuracy: 0.9487 - val_loss: 1.3419 - val_accuracy: 0.7000\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0666 - accuracy: 0.9487 - val_loss: 1.3435 - val_accuracy: 0.7000\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0661 - accuracy: 0.9573 - val_loss: 1.3388 - val_accuracy: 0.7000\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 46ms/step - loss: 0.0664 - accuracy: 0.9573 - val_loss: 1.3397 - val_accuracy: 0.7000\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.0664 - accuracy: 0.9573 - val_loss: 1.3420 - val_accuracy: 0.7000\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.0661 - accuracy: 0.9573 - val_loss: 1.3420 - val_accuracy: 0.7000\n",
      "Meilleure epoch: 36\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=200, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Meilleure epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/36\n",
      "/home/rahima/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4869: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n",
      "4/4 [==============================] - 1s 100ms/step - loss: 3.4865 - accuracy: 0.0855 - val_loss: 3.2511 - val_accuracy: 0.2667\n",
      "\n",
      "Epoch 00001: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 2/36\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.9206 - accuracy: 0.3333 - val_loss: 3.0745 - val_accuracy: 0.2333\n",
      "\n",
      "Epoch 00002: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 3/36\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.5627 - accuracy: 0.4017 - val_loss: 3.0011 - val_accuracy: 0.2333\n",
      "\n",
      "Epoch 00003: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 4/36\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.3016 - accuracy: 0.4444 - val_loss: 2.8632 - val_accuracy: 0.3667\n",
      "\n",
      "Epoch 00004: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 5/36\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 2.0650 - accuracy: 0.5812 - val_loss: 2.6754 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00005: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 6/36\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.8666 - accuracy: 0.6325 - val_loss: 2.5157 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00006: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 7/36\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.6893 - accuracy: 0.7009 - val_loss: 2.3824 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00007: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 8/36\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.5383 - accuracy: 0.7350 - val_loss: 2.2624 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00008: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 9/36\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.4076 - accuracy: 0.7607 - val_loss: 2.1498 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00009: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 10/36\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.2717 - accuracy: 0.7778 - val_loss: 2.0588 - val_accuracy: 0.4333\n",
      "\n",
      "Epoch 00010: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 11/36\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.1664 - accuracy: 0.7778 - val_loss: 2.0030 - val_accuracy: 0.4667\n",
      "\n",
      "Epoch 00011: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 12/36\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.0696 - accuracy: 0.8120 - val_loss: 1.9423 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00012: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 13/36\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.9718 - accuracy: 0.8205 - val_loss: 1.8760 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00013: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 14/36\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.8942 - accuracy: 0.8376 - val_loss: 1.8106 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00014: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 15/36\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.8221 - accuracy: 0.8632 - val_loss: 1.7453 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00015: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 16/36\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 0.7582 - accuracy: 0.8632 - val_loss: 1.6634 - val_accuracy: 0.5333\n",
      "\n",
      "Epoch 00016: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 17/36\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.6976 - accuracy: 0.8632 - val_loss: 1.5933 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00017: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 18/36\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.6501 - accuracy: 0.8803 - val_loss: 1.5459 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00018: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 19/36\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6048 - accuracy: 0.8974 - val_loss: 1.4904 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00019: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 20/36\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5607 - accuracy: 0.9145 - val_loss: 1.4601 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00020: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 21/36\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5207 - accuracy: 0.9231 - val_loss: 1.4280 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00021: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 22/36\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4894 - accuracy: 0.9231 - val_loss: 1.3863 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00022: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 23/36\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4552 - accuracy: 0.9487 - val_loss: 1.3636 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00023: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 24/36\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4285 - accuracy: 0.9487 - val_loss: 1.3391 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00024: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 25/36\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4069 - accuracy: 0.9487 - val_loss: 1.3056 - val_accuracy: 0.5667\n",
      "\n",
      "Epoch 00025: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 26/36\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3805 - accuracy: 0.9487 - val_loss: 1.2635 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00026: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 27/36\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.3599 - accuracy: 0.9487 - val_loss: 1.2294 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00027: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 28/36\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3428 - accuracy: 0.9487 - val_loss: 1.2154 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00028: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 29/36\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3252 - accuracy: 0.9487 - val_loss: 1.2118 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00029: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 30/36\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.3070 - accuracy: 0.9402 - val_loss: 1.2030 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00030: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 31/36\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.2948 - accuracy: 0.9573 - val_loss: 1.1915 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00031: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 32/36\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2820 - accuracy: 0.9573 - val_loss: 1.1782 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00032: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 33/36\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.2644 - accuracy: 0.9573 - val_loss: 1.1510 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00033: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 34/36\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2546 - accuracy: 0.9573 - val_loss: 1.1305 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00034: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 35/36\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2467 - accuracy: 0.9573 - val_loss: 1.1262 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00035: saving model to training_keras_hypermodel/cp.ckpt\n",
      "Epoch 36/36\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.2361 - accuracy: 0.9487 - val_loss: 1.1176 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00036: saving model to training_keras_hypermodel/cp.ckpt\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8bdc33d370>"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "\"\"\"\n",
    "checkpoint_path = \"training_keras_hypermodel/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\"\"\"\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(X_train, y_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2/2 [==============================] - 0s 5ms/step - loss: 1.4920 - accuracy: 0.5714\n",
      "[test loss, test accuracy]: [1.491957426071167, 0.5714285969734192]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 300)               0         \n_________________________________________________________________\ndense (Dense)                (None, 440)               132440    \n_________________________________________________________________\ndense_1 (Dense)              (None, 90)                39690     \n_________________________________________________________________\ndense_2 (Dense)              (None, 33)                3003      \n=================================================================\nTotal params: 175,133\nTrainable params: 175,133\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hypermodel.summary()"
   ]
  },
  {
   "source": [
    "## Prdire\n",
    "<br/>\n",
    "Les sorties sont des probabilites"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.2670649 , 0.0717662 , 0.20869943, ..., 0.39284045, 0.42900172,\n",
       "        0.16672283],\n",
       "       [0.22466317, 0.9612142 , 0.54913586, ..., 0.82641375, 0.38262457,\n",
       "        0.32160655],\n",
       "       [0.21008238, 0.40869057, 0.1488249 , ..., 0.3857499 , 0.36714917,\n",
       "        0.30379176],\n",
       "       ...,\n",
       "       [0.16375619, 0.3431636 , 0.99659586, ..., 0.7385855 , 0.1466837 ,\n",
       "        0.27672613],\n",
       "       [0.36131233, 0.9968251 , 0.8556698 , ..., 0.94515413, 0.18988153,\n",
       "        0.23535505],\n",
       "       [0.14269277, 0.2518988 , 0.9954884 , ..., 0.67510533, 0.1142939 ,\n",
       "        0.37323713]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "predictions = hypermodel.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'gestational_diabetes_risk_factors'"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "val_label_cor.iloc[np.argmax(predictions[0])]['intent_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"what's diabetes\"\n",
    "dd = np.array([pr.wordvec(text)])\n",
    "pp = hypermodel.predict(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'define_diabetes'"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "val_label_cor.iloc[np.argmax(pp[0])]['intent_label']"
   ]
  },
  {
   "source": [
    "## Enregistrer les modles pour les rutiliser \n",
    "<br/>\n",
    "\n",
    "On enregesitre la totalite du modele"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.save(\"saved_models_vars/bot_test_model.h5\")\n",
    "hypermodel.save(\"saved_models_vars/bot_hypermodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(val_label_cor, open('saved_models_vars/val_label_cor.pckl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}